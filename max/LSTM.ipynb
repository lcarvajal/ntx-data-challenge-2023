{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Requirement already satisfied: torch in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\maximilian\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import dirname, join as pjoin\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.pyplot import plot,figure\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get raw and filtered data for S002\n",
    "\n",
    "data_dir = pjoin(os.getcwd(),'..', 'dataset_phase1_ntx23')\n",
    "S002_raw_mat_fname = pjoin(data_dir, 'train_S002_night1_hackathon_raw.mat')\n",
    "S002_filt_mat_fname = pjoin(data_dir, 'train_S002_night1_hackathon_filt.mat')\n",
    "\n",
    "S002_raw_mat_contents = sio.loadmat(S002_raw_mat_fname)\n",
    "S002_filt_mat_contents = sio.loadmat(S002_filt_mat_fname)\n",
    "\n",
    "\n",
    "S002_raw_EEG = S002_raw_mat_contents['EEG']\n",
    "S002_filt_EEG = S002_filt_mat_contents['EEG']\n",
    "\n",
    "fs = S002_raw_EEG['srate'].item()[0].item()\n",
    "\n",
    "raw_data = S002_raw_EEG['data'].item()[0]\n",
    "filt_data_1_to_35Hz = S002_filt_EEG['data'].item()[0]\n",
    "filt_data_4_to_18Hz = S002_filt_EEG['data'].item()[1]\n",
    "\n",
    "time = S002_raw_EEG['times'].item()[0]\n",
    "\n",
    "#Load marker data for S002 into dataframe\n",
    "marker_fname = pjoin(os.getcwd(),\"..\",\"dataset_phase1_ntx23\",\"train_S002_labeled.csv\")\n",
    "markers_df = pd.read_csv(marker_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4965399\n",
      "663\n",
      "7500\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "window_len = 7500\n",
    "print(len(filt_data_1_to_35Hz))\n",
    "num_epochs = int(np.ceil(len(filt_data_1_to_35Hz)/window_len))\n",
    "print(num_epochs)\n",
    "filt_data_1_to_35Hz_epochs = [filt_data_1_to_35Hz[i*window_len:(i*window_len+window_len)] for i in range(0,num_epochs-1)]\n",
    "\n",
    "\n",
    "print(len(filt_data_1_to_35Hz_epochs[5]))\n",
    "last_epoch = filt_data_1_to_35Hz[(num_epochs-1)*window_len:]\n",
    "zeros = np.zeros((1,window_len - len(last_epoch)))\n",
    "last_epoch = np.append(last_epoch,zeros)\n",
    "print(len(last_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4965399,)\n"
     ]
    }
   ],
   "source": [
    "labels_to_use:List[str]=[\"SS1\",\"REM1\",\"REM0\",\"K1\"]\n",
    "\n",
    "timestamp_labels = np.zeros((len(filt_data_1_to_35Hz)))\n",
    "print(timestamp_labels.shape)\n",
    "for i,curr_label in enumerate(labels_to_use):\n",
    "    idx = markers_df.loc[markers_df[curr_label]>0,['Timestamp']].values\n",
    "    timestamp_labels[idx] = i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetEEG(Dataset):\n",
    "    def __init__(self,signal:np.ndarray,labels_df:pd.DataFrame, transform, window_len:int=7500, labels_to_use:List[str]=[\"SS1\",\"REM1\",\"REM0\",\"K1\"]):\n",
    "        super().__init__()\n",
    "        self.signal = signal\n",
    "        self.labels_df = labels_df \n",
    "        self.signal_len = len(signal)\n",
    "        self.timestamp_labels = np.zeros((self.signal_len))\n",
    "        self.window_len = window_len\n",
    "        self.num_epochs = int(np.ceil(self.signal_len/self.window_len))\n",
    "        self.labels_to_use = labels_to_use\n",
    "        self.transform = transform\n",
    "\n",
    "        #split signal into epochs of length window_len\n",
    "        self.epochs = [self.signal[i*self.window_len:(i*self.window_len+self.window_len)] for i in range(0,self.num_epochs-1)]\n",
    "        last_epoch = self.signal[(self.num_epochs-1)*self.window_len:]\n",
    "\n",
    "        #Add zero padding to final epoch and append to epochs list\n",
    "        if len(last_epoch) < window_len:\n",
    "            zeros = np.zeros((1,window_len - len(last_epoch)))\n",
    "            last_epoch = np.append(last_epoch,zeros)\n",
    "        self.epochs.append(last_epoch)\n",
    "\n",
    "        self.GetTimestampLabels()\n",
    "        self.epoch_labels = [self.timestamp_labels[i*self.window_len:(i*self.window_len+self.window_len)] for i in range(0,self.num_epochs-1)]\n",
    "        last_epoch_labels = self.timestamp_labels[(self.num_epochs-1)*self.window_len:]\n",
    "        #Add zero padding to final epoch and append to epochs list\n",
    "        \n",
    "        if len(last_epoch_labels) < window_len:\n",
    "            last_epoch_labels = np.append(last_epoch_labels,zeros)\n",
    "        self.epoch_labels.append(last_epoch_labels)\n",
    "        \n",
    "\n",
    "    def GetTimestampLabels(self):\n",
    "        \"\"\"Get labels for every timestamp/sample\"\"\"\n",
    "        for i,curr_label in enumerate(self.labels_to_use):\n",
    "            idx = self.labels_df.loc[self.labels_df[curr_label]>0,['Timestamp']].values\n",
    "            self.timestamp_labels[idx] = i+1\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_epochs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #if self.transform:\n",
    "        #    return self.transform(torch.tensor(self.epochs[index])), torch.tensor(self.epoch_labels[index])\n",
    "        return torch.tensor(self.epochs[index]), torch.tensor(self.epoch_labels[index], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetEEG(\n",
    "                 signal=filt_data_1_to_35Hz, #[:len(filt_data_1_to_35Hz)-399],\n",
    "                 labels_df=markers_df,\n",
    "                 transform = None,\n",
    "                 window_len=7500,\n",
    "                 labels_to_use=[\"SS1\",\"REM1\",\"REM0\",\"K1\"]\n",
    "                 )\n",
    "\n",
    "split_ind = int(len(data)*0.8)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "-Figure out issue with using full signal length (right now uncommenting \"[:len(filt_data_1_to_35Hz)-399],\" and removing last 399 samples)\n",
    "-Figure out need for final MLP layer after LSTM\n",
    "-Create training loop\n",
    "    -Add optimizer\n",
    "    -Add loss function\n",
    "    -Add backprop\n",
    "-Create metrics for performance assessment (WandB?)\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        out = self.relu(output)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "\n",
    "input_size = 1 #number of features\n",
    "hidden_size = 2 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = len(labels_to_use) #number of output classes\n",
    "\n",
    "\n",
    "lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, window_len)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()   # BCELoss for regression\n",
    "optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  for step,(x,y) in enumerate(train_dataloader):\n",
    "    x = x.unsqueeze(dim=2)\n",
    "    outputs = lstm1.forward(x) #forward pass\n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    "    y = torch.nn.functional.one_hot(y,num_classes).float()\n",
    "    \n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs, y)\n",
    "  \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    "  \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "  if epoch % 10 == 0:\n",
    "    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurotech_data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
